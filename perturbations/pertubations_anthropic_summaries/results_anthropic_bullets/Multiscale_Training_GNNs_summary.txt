Here's the summary converted into bullet points, preserving all information:

*   The paper introduces multiscale training methods to improve the computational efficiency of Graph Neural Networks (GNNs) while maintaining or improving performance.
*   Three main approaches are proposed:
    *   **Coarse-to-Fine**: Training on progressively smaller pooled graphs.
    *   **Sub-to-Full**: Training on expanding subgraphs.
    *   **Multiscale Gradients Computation**: Approximating fine-scale gradients using multiple resolution levels.
*   The key insight is that GNN weight matrices are size-independent.
*   This size independence allows training on reduced graphs with fewer nodes and edges to significantly decrease computational costs.
*   Experiments were conducted across multiple datasets: OGBN-Arxiv, OGBN-MAG, ShapeNet, PPI, etc.
*   Experiments were also conducted across multiple architectures: GCN, GIN, GAT.
*   These methods achieved comparable or superior accuracy to standard training.
*   These methods reduced FLOPs and training time by up to 80%.
*   The approach is general.
*   The approach is applicable to various GNN architectures.
*   The approach is applicable to various pooling strategies.
*   The approach is applicable to both transductive and inductive learning tasks.