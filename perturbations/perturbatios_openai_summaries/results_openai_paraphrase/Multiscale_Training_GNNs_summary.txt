This paper presents a novel multiscale approach designed to enhance the efficiency of training Graph Neural Networks (GNNs) when dealing with extensive and densely interconnected graphs. The core idea is to circumvent the need for computations on the entire graph during the majority of the training process.

The framework constructs a series of less computationally intensive proxy problems through the use of graph coarsening techniques, subgraph extraction, or a combination of both. Crucially, it facilitates the transfer of learned parameters across these different scales, capitalizing on the property that GNN parameters are invariant to the number of nodes.

The researchers put forth three distinct training methodologies:
1.  **Coarse-to-Fine Optimization:** This strategy leverages solutions obtained on coarser graph representations to seed and accelerate training on finer scales.
2.  **Subgraph-to-Full Training:** This method involves progressively training on increasingly larger subgraphs, culminating in training on the complete graph.
3.  **Multiscale Gradient Computation:** This approach integrates gradients and loss calculations from various scales using a telescoping-sum principle, thereby minimizing computationally demanding updates at the finest scale.

Theoretical underpinnings are offered within a linearized context, drawing connections between subgraph-based training and concepts like sketching and subspace embeddings. An analysis is also provided to demonstrate how reductions in node and edge counts lead to decreased floating-point operations (FLOPs) and memory requirements.

Experimental evaluations conducted on both transductive and inductive benchmark datasets, including OGBN-Arxiv, OGBN-MAG, ShapeNet, and PPI, reveal significant reductions in runtime and memory consumption. These improvements are achieved while maintaining or even surpassing predictive accuracy. The optimal coarsening strategy is observed to be dataset-dependent.