This article introduces a *closed testing procedure* designed to determine the appropriate extent of updates for a pre-existing logistic regression prediction model that exhibits subpar performance in a new population. The approach evaluates a range of update possibilities, organized by escalating complexity: maintaining the original model, adjusting solely the intercept (termed recalibration-in-the-large), modifying both the intercept and a global slope (referred to as recalibration or logistic calibration), or completely re-estimating all coefficients (model revision). This method employs a series of likelihood ratio tests, beginning with the most intricate model (revision). It progresses towards selecting less complex updates only when the data provides sufficient evidence, thereby minimizing unwarranted overfitting while aiming to manage the overall Type I error rate. In three clinical examples, the procedure frequently favored simpler updates, such as adjusting only the intercept for the prostate cancer model. This occurred even when individual tests might have indicated a full revision, which subsequently displayed signs of overfitting during internal validation. A simulation study demonstrated that the procedure's Type I error rate remained close to the intended level (slightly elevated in extremely small samples) and generally resulted in a lower mean squared error for coefficient estimates compared to consistently undertaking a full model revision, particularly when the ratio of events to variables was low.