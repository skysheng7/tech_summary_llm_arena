Here is the reordered summary:

The article proposes a *closed testing procedure* to choose how extensively to update an existing logistic-regression prediction model when it performs poorly in a new population. It considers a hierarchy of updating options of increasing complexity: keep the original model, update only the intercept (recalibration-in-the-large), update intercept plus a global slope (recalibration/logistic calibration), or re-estimate all coefficients (model revision). The method uses a sequence of likelihood ratio tests starting with the most complex model (revision) and, only if justified by the data, moves toward selecting simpler updates—thereby limiting unnecessary overfitting while approximately controlling the overall Type I error. In three clinical case studies, the procedure often selected parsimonious updates (e.g., intercept-only adjustment for the prostate cancer model) even when separate tests might have suggested full revision that then showed signs of overfitting on internal validation. A simulation study found the procedure’s Type I error was close to nominal (slightly high in very small samples) and that it tended to yield lower mean squared error in coefficient estimates than always doing full model revision, especially at low events-per-variable ratios.