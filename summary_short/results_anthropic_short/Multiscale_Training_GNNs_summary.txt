# Summary of "Towards Efficient Training of Graph Neural Networks: A Multiscale Approach"

This paper introduces multiscale training methods to improve the computational efficiency of Graph Neural Networks (GNNs) while maintaining or improving performance. The authors propose three main approaches: **Coarse-to-Fine** (training on progressively smaller pooled graphs), **Sub-to-Full** (training on expanding subgraphs), and **Multiscale Gradients Computation** (approximating fine-scale gradients using multiple resolution levels). The key insight is that GNN weight matrices are size-independent, allowing training on reduced graphs with fewer nodes and edges to significantly decrease computational costs. Experiments across multiple datasets (OGBN-Arxiv, OGBN-MAG, ShapeNet, PPI, etc.) and architectures (GCN, GIN, GAT) demonstrate that these methods achieve comparable or superior accuracy to standard training while reducing FLOPs and training time by up to 80%. The approach is general and applicable to various GNN architectures, pooling strategies, and both transductive and inductive learning tasks.