The paper introduces a multiscale framework to make training Graph Neural Networks (GNNs) more efficient on large, highly connected graphs by avoiding full-graph computation for most of training. It builds a hierarchy of cheaper surrogate problems using graph coarsening and/or subgraph extraction, then transfers the learned weights across scales because GNN parameters are independent of the number of nodes. The authors propose three training strategies: (1) coarse-to-fine optimization that uses coarse solutions to initialize finer training, (2) subgraph-to-full training that grows from small subgraphs to the full graph, and (3) multiscale gradient computation that combines gradients/losses across scales via a telescoping-sum idea to reduce expensive fine-scale updates. They provide limited theory in a linearized setting linking subgraph training to sketching/subspace embeddings and analyze how reduced node/edge counts lower FLOPs and memory. Experiments on transductive and inductive benchmarks (e.g., OGBN-Arxiv, OGBN-MAG, ShapeNet, PPI) show substantial runtime and memory reductions while matching or sometimes improving predictive performance, with the best coarsening strategy depending on the dataset.