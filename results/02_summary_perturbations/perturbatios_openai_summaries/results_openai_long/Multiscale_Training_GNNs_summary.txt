This research paper presents a novel and innovative multiscale framework designed to significantly enhance the efficiency of training Graph Neural Networks (GNNs), particularly when these networks are applied to very large graphs that exhibit a high degree of connectivity. The core idea behind this framework is to circumvent the computationally intensive process of performing full-graph computations for the majority of the training duration. Instead, the paper proposes to construct a series of more computationally manageable, or "cheaper," surrogate problems. This construction is achieved through the judicious application of graph coarsening techniques and/or the extraction of subgraphs.

The fundamental premise enabling this multiscale approach is the inherent characteristic of GNN parameters, which are notably independent of the total number of nodes present in the graph. This crucial observation allows for the effective transfer of learned weights across these different scales of representation. Essentially, what is learned at a coarser level can serve as a valuable starting point or even a direct contributor to the learning process at finer levels of detail.

To implement this multiscale framework, the authors put forth three distinct training strategies. The first strategy, termed coarse-to-fine optimization, leverages the solutions obtained from training on coarser representations of the graph to effectively initialize and guide the training process on finer, more detailed representations. This sequential approach aims to accelerate convergence by building upon prior learning. The second proposed strategy is subgraph-to-full training. This method adopts an incremental approach, commencing training on smaller, manageable subgraphs and progressively expanding the scope of computation to encompass larger subgraphs and ultimately the entire graph. This allows for a gradual adaptation and learning process. The third training strategy is multiscale gradient computation. This approach intelligently combines gradients and loss calculations across different scales of the graph. It employs a concept analogous to a telescoping sum, where contributions from various scales are aggregated in a way that specifically aims to reduce the frequency and computational burden associated with expensive updates at the finest, most detailed scale.

While the theoretical underpinnings provided are somewhat limited, they are presented within a linearized setting. In this simplified context, the authors draw connections between the training of GNNs on subgraphs and established concepts such as sketching and subspace embeddings. Furthermore, the paper includes an analysis that quantifies the benefits of reduced node and edge counts, demonstrating how these reductions directly translate into lower computational costs in terms of Floating-point Operations (FLOPs) and memory requirements.

The practical efficacy of this multiscale framework is substantiated through a series of experiments. These experiments were conducted on a variety of benchmark datasets, encompassing both transductive and inductive learning tasks. Notable examples of these datasets include OGBN-Arxiv, OGBN-MAG, ShapeNet, and PPI. The results of these experiments consistently demonstrate substantial reductions in both training runtime and memory consumption. Crucially, this efficiency gain is achieved while maintaining, and in some instances even surpassing, the predictive performance achieved by traditional, full-graph training methods. The authors also note that the optimal strategy for graph coarsening, in terms of achieving the best performance, appears to be dataset-dependent, highlighting the need for adaptive selection of coarsening methods.