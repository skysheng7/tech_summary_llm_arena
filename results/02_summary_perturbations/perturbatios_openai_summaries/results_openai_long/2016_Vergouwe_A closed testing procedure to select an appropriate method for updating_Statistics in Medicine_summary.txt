Here's a longer, more redundant version of the summary, restating ideas and adding explanations without introducing new facts:

This article puts forth a specific strategy, referred to as a *closed testing procedure*, which serves as a method for making the crucial decision of how extensively to modify or update a pre-existing prediction model that is based on logistic regression. This decision-making process becomes particularly important when it has been observed that this established model is not performing as well as it should when applied to a novel or different population. The proposed procedure systematically considers a range of potential updating choices, arranged in an ascending order of complexity, or a hierarchy of updating options, if you will. These options, starting from the least complex and progressing to the most intricate, are as follows: the simplest choice is to simply retain and continue using the original, unmodified model as it was. Moving up in complexity, the next option involves updating just the intercept term of the model, a technique often referred to as recalibration-in-the-large. Following this, a more involved approach would be to update both the intercept and a single, overarching slope coefficient that applies globally across all predictor variables. This is sometimes called recalibration or logistic calibration. Finally, at the highest level of complexity, the procedure allows for the complete re-estimation of all the model's coefficients.

The core of this method relies on a systematic progression of statistical tests, specifically a sequence of likelihood ratio tests. This testing sequence begins by evaluating the most complex updating scenario, which is the full model revision where all coefficients are re-estimated. Only if the data provides sufficient statistical evidence to support this most complex approach, meaning it is truly justified by the observations at hand, will the procedure then proceed to consider and potentially select simpler updating strategies. This deliberate backward movement towards simpler updates is designed to prevent the unnecessary and potentially detrimental overfitting of the model to the new data. Overfitting occurs when a model becomes too tailored to the specific nuances of the data it's being trained on, leading to poor performance on new, unseen data. By only moving to simpler updates when the data warrants it, the procedure aims to limit this risk of overfitting. Simultaneously, this closed testing procedure also strives to maintain approximate control over the overall rate of Type I errors. A Type I error, in this context, is essentially the mistake of concluding that an update is necessary when, in reality, no update is truly needed.

The practical utility and effectiveness of this proposed closed testing procedure have been demonstrated through an examination of three distinct clinical case studies. In these real-world scenarios, the procedure frequently resulted in the selection of updates that were parsimonious, meaning they were the simplest effective options. For instance, in the case of the prostate cancer model, the procedure often favored a straightforward intercept-only adjustment. This is noteworthy because, in some of these same studies, if separate, less controlled tests had been conducted, they might have erroneously suggested a full model revision. However, when these potentially over-ambitious full revisions were then put to an internal validation process, they often exhibited clear signs of overfitting, confirming the benefit of the closed testing procedure's more conservative approach. Further bolstering the confidence in this method, a simulation study was undertaken. This simulation investigated the procedure's performance under controlled conditions. The findings from this simulation indicated that the procedure's Type I error rate remained close to the target, or nominal, rate. It