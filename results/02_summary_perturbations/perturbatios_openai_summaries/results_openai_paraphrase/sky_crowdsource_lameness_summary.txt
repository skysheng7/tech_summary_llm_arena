This study investigated a remote, comparative approach for evaluating lameness in dairy cows. Participants viewed pairs of videos featuring cows walking, presented side-by-side, and were instructed to determine which cow exhibited greater lameness and to what extent, using a scale from -3 to +3. Each of 50 crowd workers performed 11 brief assignments, each consisting of 10 video pairs. Their judgments were then compared to evaluations made by five seasoned lameness specialists. While individual crowd workers demonstrated a moderate level of consensus among themselves, the aggregated crowd ratings closely matched the average expert ratings across 88 comparative assessments (intraclass correlation coefficients ranging from approximately 0.89 to 0.91). Furthermore, employing additional data filtering or clustering techniques did not significantly enhance the agreement between crowd and expert assessments. The researchers also observed that aggregating the ratings of approximately 10 crowd workers per task was sufficient to achieve a robust agreement with experts (ICC exceeding 0.80), with minimal gains in agreement beyond this number. The authors therefore propose that crowdsourced, side-by-side video comparisons offer a swift and economical method for estimating herd lameness levels and for creating extensive, labeled datasets essential for developing automated computer vision systems for lameness detection.