This paper explores novel memory-saving reversible designs for large language models (LLMs). These architectures eliminate the need to store intermediate activation values by recreating hidden states during the backward pass, leveraging time-reversible dynamics akin to differential equations. The authors detail three distinct reversible update mechanisms: midpoint, leapfrog (second-order/wave-equation), and Hamiltonian/symplectic dynamics. They further investigate the conditions under which these techniques exhibit stability in both forward and backward computations. A significant advantage of these models is their activation memory footprint, which remains largely consistent as model depth increases. This allows for considerably larger training batch sizes (approximately ten times greater across multiple GPUs), frequently leading to enhanced throughput, even with some additional computational effort for reconstruction. The research also introduces a practical "retrofit" approach to transform standard, non-reversible pretrained transformers into their reversible counterparts. This involves an approximation step followed by a brief fine-tuning phase to synchronize the outputs with those of the original model. Empirical results on GPT-2 (small and large variants), along with conversions of TinyLlama and SmolLM2, demonstrate equivalent or occasionally superior loss and zero-shot benchmark performance, accompanied by a substantial reduction in memory consumption.