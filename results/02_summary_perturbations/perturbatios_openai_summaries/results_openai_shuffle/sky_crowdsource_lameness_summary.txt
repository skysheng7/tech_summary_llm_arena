Here is the reordered summary:

The article tests a remote, comparison-based way to assess dairy cow lameness by showing two walking-cow videos side-by-side and asking online crowd workers to judge which cow is more lame and by how much (−3 to +3). Fifty crowd workers completed each of 11 short tasks (10 video pairs each), and their results were compared with ratings from five highly experienced lameness assessors. Individual crowd workers showed moderate agreement with each other, but the average crowd rating agreed very closely with the average expert rating across 88 test comparisons (intraclass correlation ≈ 0.89–0.91), and extra data filtering/clustering did not meaningfully improve this crowd-vs-expert agreement. The authors also found that averaging about 10 crowd workers per task was enough to achieve strong agreement with experts (ICC > 0.80), with little improvement beyond 10 workers. They conclude that crowdsourced pairwise video comparisons could provide a fast, low-cost method to estimate lameness in herds and to generate large labeled datasets for training automated computer-vision lameness detection systems.