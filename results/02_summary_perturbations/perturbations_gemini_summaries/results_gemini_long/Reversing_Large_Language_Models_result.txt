This paper presents novel architectural designs for Large Language Models (LLMs) that are specifically engineered to be memory-efficient and, importantly, reversible. These innovative architectures draw their conceptual inspiration directly from principles observed in the realm of physics. The core idea behind these memory-efficient designs is to achieve substantial reductions in the amount of memory needed during the training process. This reduction in memory consumption is accomplished by strategically reconstructing intermediate activations, which are the outputs of various layers within the model, specifically during the backpropagation phase of training. By being able to recreate these intermediate activations on-the-fly rather than storing them, the models can operate with considerably larger batch sizes. This capability of handling larger batches, in turn, leads to a significant improvement in the overall throughput of the training process, meaning more data can be processed in a given amount of time.

Furthermore, the authors have developed a practical methodology that enables the conversion of existing Large Language Models, which are typically not designed to be reversible, into reversible architectures. This conversion is achieved through a fine-tuning process, thereby making this memory-saving approach accessible and applicable to a wide range of pre-existing LLMs. The experimental results detailed in the paper strongly support the efficacy of these reversible models. They demonstrate that these newly proposed reversible models are capable of achieving performance levels that are either on par with, or even superior to, standard, non-reversible LLMs. Crucially, this comparable or enhanced performance is attained while simultaneously and substantially lowering the memory footprint of the models. In essence, this research offers a viable and scalable pathway for making the training and fine-tuning of Large Language Models more efficient, which is particularly beneficial and important when operating within environments that have limited computational resources. The work provides a much-needed solution for scaling up LLM development and deployment, especially in scenarios where memory is a bottleneck.