This article puts forth a new methodology, specifically a **closed testing procedure**, designed to help researchers and practitioners make a well-informed decision regarding which particular method to employ when it comes time to update predictive models that have already been developed and established. The core purpose of this proposed procedure is to identify the *most suitable* or *most appropriate* way to adapt these pre-existing prediction models when they are intended to be used on *new populations* of data, distinct from the original populations for which they were initially built.

To achieve this goal, the proposed closed testing procedure employs a structured and methodical approach. It systematically evaluates a range of potential updating methods, beginning with the simplest and least invasive modifications. These minimal adjustments are referred to as "recalibration in the large," which essentially involves making broad, systematic shifts to the model's predictions without altering its underlying structure. The procedure then progresses to consider increasingly sophisticated and complex methods of updating. This escalation continues all the way up to the most comprehensive form of updating, which would involve a *full model revision*, meaning the entire structure and parameters of the original prediction model might be re-examined and potentially changed. Throughout this hierarchical testing, the procedure is meticulously designed to maintain control over a specific statistical risk known as the **Type I error**. The Type I error, in this context, refers to the erroneous conclusion that an updating method is necessary or beneficial when, in reality, it is not. By controlling for this error, the procedure aims to ensure that decisions to update are based on genuine improvements rather than random chance.

This carefully orchestrated approach offers a crucial benefit: it strikes a sensible **balance**. On one hand, there is a clear and often pressing **need for updating** prediction models when they are applied to new populations. Models developed on one group of people or under one set of conditions may not perform optimally when deployed in a different context. On the other hand, there is a significant **risk of overfitting**. Overfitting occurs when a model becomes too tailored to the specific nuances of a particular dataset, to the point where it performs poorly on new, unseen data. This risk is particularly pronounced, or *amplified*, when dealing with **smaller sample sizes**. In such situations, even minor adjustments can lead to a model that captures noise rather than true underlying patterns. The closed testing procedure helps to mitigate this danger.

The practical utility and effectiveness of this closed testing procedure are further underscored by the authors' demonstration. They illustrate how the procedure works using **three distinct clinical examples**. These real-world scenarios serve to concretely show the step-by-step application of the methodology. Crucially, through these examples, the authors are able to **demonstrate its effectiveness in selecting parsimonious update methods**. Parsimonious methods are those that are simpler and involve fewer changes. The procedure is shown to be adept at identifying these simpler, more efficient updates when the **evidence for revision** – meaning the data does not strongly indicate the need for a substantial overhaul – is **limited**. In essence, when the data doesn't scream for a major change, the procedure is able to identify a modest, effective adjustment.

In summary, the overarching objective and ultimate aim of this proposed **closed testing procedure** are twofold. Firstly, it is designed with the explicit intention to **prevent overfitting**. By systematically and conservatively testing update methods, it guards against the temptation to make overly complex changes that might harm generalization. Secondly,