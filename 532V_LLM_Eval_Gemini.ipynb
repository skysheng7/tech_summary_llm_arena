{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key='AIzaSyBVmykEGkH8zT0DzMyIME6O7TFgwy2rFiI')"
      ],
      "metadata": {
        "id": "WH1HUoijPK0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up Gemini client"
      ],
      "metadata": {
        "id": "abqMFAvkXtfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def send_request(prompt, max_tokens):\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash-lite', # or gemini-2.5-flash\n",
        "            contents=[prompt],\n",
        "            config={\n",
        "                'max_output_tokens': max_tokens,\n",
        "                'temperature': 1.0,\n",
        "                'thinking_config': {\n",
        "                    'include_thoughts': True, # Set to True if you want to see the reasoning\n",
        "                    'thinking_budget': 0   # Minimum budget is 0 if we don't use a thinking model\n",
        "                }\n",
        "            },\n",
        "        )\n",
        "        if response:\n",
        "            print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
        "            print(f\"Thinking tokens: {response.usage_metadata.thoughts_token_count}\") # <--- This one\n",
        "            print(f\"Output tokens (Answer): {response.usage_metadata.candidates_token_count}\")\n",
        "            print(f\"Total tokens used: {response.usage_metadata.total_token_count}\")\n",
        "            return response.text\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    return ''\n",
        "\n",
        "send_request(prompt=\"How are you?\", max_tokens=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "kaKBRrpHYyXf",
        "outputId": "bf9bcc72-27cb-4520-e8b5-c910fbb945af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 5\n",
            "Thinking tokens: None\n",
            "Output tokens (Answer): 39\n",
            "Total tokens used: 44\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am a large language model, trained by Google. I don\\'t have feelings or emotions, so I don\\'t experience \"how are you?\" in the same way a human does.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}