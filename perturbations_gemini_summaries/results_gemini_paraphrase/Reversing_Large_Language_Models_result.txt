Drawing inspiration from physics, this research presents memory-saving and reversible designs for Large Language Models (LLMs). By regenerating intermediate activations during the backpropagation process, these designs minimize memory usage, which in turn enables larger batch sizes and boosts training speed. Furthermore, the researchers provide a technique for transforming existing LLMs that are not inherently reversible into reversible ones via fine-tuning, thus enhancing the practicality of their proposed method. Empirical evaluations demonstrate that these reversible models attain performance levels on par with or exceeding those of conventional LLMs, all while substantially lowering memory demands. This contribution paves the way for more scalable and efficient training and fine-tuning of LLMs, particularly in situations with limited computational resources.