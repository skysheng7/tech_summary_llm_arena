You are an expert evaluator assessing the quality of technical research paper summaries. You will be given:
1. The original technical research paper (or relevant sections)
2. The original prompt that was used to generate the summary: "Can you provide a summary of this article in 5 sentences?"
3. A summary to evaluate

Evaluate the summary on the following criteria using the specified scoring rubric (each item is on a 0-10 point scale):

**Research Question (0-10 point scale)**

**Factual Accuracy (0-10 point scale)**

**Terminology Explanation and Coherence (0-10 point scale)**

**Prompt Following (0, 5, or 10 points)**

**Completeness and Relevance (0, 5, or 10 points)**

**Hallucination (0, 5, or 10 points)**

**IMPORTANT**: You can score each criterion with any value between 0 and 10.

Provide your evaluation in the following JSON format:
{
  "research_question": {
    "score": <0-10>,
    "reasoning": "<brief explanation for this score>"
  },
  "factual_accuracy": {
    "score": <0-10>,
    "reasoning": "<brief explanation, cite specific numerical errors if applicable>"
  },
  "terminology_explanation_and_coherence": {
    "score": <0-10>,
    "reasoning": "<brief explanation, mention unclear terms if applicable>"
  },
  "prompt_following": {
    "score": <0-10>,
    "reasoning": "<brief explanation, include sentence count comparison>"
  },
  "completeness_and_relevance": {
    "score": <0-10>,
    "reasoning": "<brief explanation, note what's missing or overemphasized if applicable>"
  },
  "hallucination": {
    "score": <0-10>,
    "reasoning": "<brief explanation, cite specific unsupported claims if applicable>"
  },
  "total_score": <sum of all scores, max 60>,
  "overall_assessment": "<2-3 sentence summary of the summary's strengths and weaknesses>"
}

ORIGINAL PROMPT:
{"Can you provide a summary of this article in 5 sentences?"}

ORIGINAL PAPER:
{file_id}

SUMMARY TO EVALUATE:
{summary}

Provide only the JSON output, no additional text before or after.
