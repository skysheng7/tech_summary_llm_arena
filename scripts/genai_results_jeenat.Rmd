---
title: "Untitled"
author: "Jeenat Mehareen"
date: "2026-01-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
install.packages(c("readr", "dplyr", "openxlsx"))
library(readr)
library(dplyr)
library(openxlsx)
library(tidyr)
library(psych)
library(lme4)
install.packages("lmerTest")
library(lmerTest)
```

```{r}
url <- "https://raw.githubusercontent.com/skysheng7/tech_summary_llm_arena/refs/heads/main/all_judgements_meta.csv?token=GHSAT0AAAAAADTKNH4LG3UQQOJF4OIJ2CBY2LQM44Q"

judgements <- read_csv(url)
```


```{r}

df <- judgements

class(df)

# Rename paper_id 
df <- df %>%
  mutate(paper_id = as.integer(factor(paper_id, levels = unique(paper_id))))
  
  
  df <- df %>%
  rename(
    CR  = completeness_and_relevance.score,
    FA  = factual_accuracy.score,
    HL  = hallucination.score,
    PF  = prompt_following.score,
    RQ  = research_question.score,
    TEC = terminology_explanation_and_coherence.score
  )
  

```
```{r}

score_cols <- df %>%
  select(CR, FA, HL, PF, RQ, TEC)
  
  avg_scores <- df %>%
  mutate(paper_id = as.integer(paper_id)) %>%
  filter(paper_id %in% 1:20) %>%
  group_by(judge_prompt, generator_model, summary_style) %>%
  summarise(
    RQ  = mean(RQ,  na.rm = TRUE),
    FA  = mean(FA,  na.rm = TRUE),
    TEC = mean(TEC, na.rm = TRUE),
    PF  = mean(PF,  na.rm = TRUE),
    CR  = mean(CR,  na.rm = TRUE),
    HL  = mean(HL,  na.rm = TRUE),
    Total_Score = mean(RQ + FA + TEC + PF + CR + HL, na.rm = TRUE),
    .groups = "drop"
  )
```
```{r}

write.xlsx(
  avg_scores,
  file = "avg_scores_1to20.xlsx",
  sheetName = "Averages",
  overwrite = TRUE
)
```



```{r}
#minus the short one 
avg_scores1 <- df %>%
  mutate(paper_id = as.integer(paper_id)) %>%
  filter(
    paper_id %in% 1:20,
    summary_style != "short"
  ) %>%
  group_by(judge_prompt, generator_model, summary_style) %>%
  summarise(
    RQ  = mean(RQ,  na.rm = TRUE),
    FA  = mean(FA,  na.rm = TRUE),
    TEC = mean(TEC, na.rm = TRUE),
    PF  = mean(PF,  na.rm = TRUE),
    CR  = mean(CR,  na.rm = TRUE),
    HL  = mean(HL,  na.rm = TRUE),
    Total_Score = mean(RQ + FA + TEC + PF + CR + HL, na.rm = TRUE),
    .groups = "drop"
  )

```


```{r}
#only short one 
avg_scores2 <- df %>%
  mutate(paper_id = as.integer(paper_id)) %>%
  filter(
    paper_id %in% 1:20,
    summary_style == "short"
  ) %>%
  group_by(judge_prompt, generator_model, summary_style) %>%
  summarise(
    RQ  = mean(RQ,  na.rm = TRUE),
    FA  = mean(FA,  na.rm = TRUE),
    TEC = mean(TEC, na.rm = TRUE),
    PF  = mean(PF,  na.rm = TRUE),
    CR  = mean(CR,  na.rm = TRUE),
    HL  = mean(HL,  na.rm = TRUE),
    Total_Score = mean(RQ + FA + TEC + PF + CR + HL, na.rm = TRUE),
    .groups = "drop"
  )

```

```{r}
##Sanity check

df %>%
  summarise(
    n_rows = n(),
    n_papers = n_distinct(paper_id),
    n_prompts = n_distinct(judge_prompt),
    n_styles = n_distinct(summary_style),
    n_models = n_distinct(generator_model),
    expected = n_papers * n_prompts * n_styles * n_models
  )

```
```{r}

##Sanity check 
all_expected <- expand_grid(
  paper_id = sort(unique(df$paper_id)),
  judge_prompt = sort(unique(df$judge_prompt)),
  summary_style = sort(unique(df$summary_style)),
  generator_model = sort(unique(df$generator_model))
)

missing_rows <- all_expected %>%
  anti_join(df %>% select(paper_id, judge_prompt, summary_style, generator_model),
            by = c("paper_id","judge_prompt","summary_style","generator_model"))

missing_rows

```


```{r}
#ICC 
icc_one_style <- function(data, metric) {


  wide <- data %>%
    group_by(paper_id, generator_model) %>%
    summarise(val = mean(.data[[metric]], na.rm = TRUE), .groups = "drop") %>%
    pivot_wider(names_from = generator_model, values_from = val)

  # Remove if missing any models
  mat <- wide %>% select(-paper_id)
  mat_complete <- mat[complete.cases(mat), , drop = FALSE]


  if (nrow(mat_complete) < 2 || ncol(mat_complete) < 2) return(NA_real_)

  icc_out <- ICC(mat_complete)
  icc_out$results$ICC[icc_out$results$type == "ICC2"]
}

```
```{r}
metrics <- c("RQ", "FA", "TEC", "PF", "CR", "HL")

icc_results <- df %>%
  group_by(summary_style) %>%
  summarise(
    across(
      all_of(metrics),
      ~ icc_one_style(cur_data(), cur_column()),
      .names = "ICC_{.col}"
    ),
    n_papers_used = {
      wide_tmp <- cur_data() %>%
        group_by(paper_id, generator_model) %>%
        summarise(n = n(), .groups = "drop") %>%
        pivot_wider(names_from = generator_model, values_from = n)

      sum(complete.cases(wide_tmp[,-1]))
    },
    .groups = "drop"
  )

```

```{r}
#Mixed effects model
df <- df %>%
  mutate(
    Total_Score = CR + FA + HL + PF + RQ + TEC
  )


df <- df %>%
  mutate(
    summary_style   = relevel(factor(summary_style), ref = "short"),
    generator_model = relevel(factor(generator_model), ref = "gemini"),
    judge_prompt    = factor(judge_prompt),   
    paper_id        = factor(paper_id)        
  )

levels(df$summary_style)
levels(df$generator_model)


model_total <- lmer(
  Total_Score ~ summary_style + generator_model + judge_prompt +
    (1 | paper_id),
  data = df
)

anova(model_total)
summary(model_total)

```

