This article proposes a closed testing procedure to select the most appropriate method for updating previously developed prediction models when applied to new populations. The procedure systematically tests increasingly complex updating methods, starting from minimal adjustments (recalibration in the large) to full model revision, while controlling for Type I error. This approach balances the need for updating with the risk of overfitting, particularly in smaller sample sizes. The authors illustrate the procedure with three clinical examples and demonstrate its effectiveness in selecting parsimonious update methods when evidence for revision is limited. The closed testing procedure aims to prevent overfitting and ensure more reliable predictions when adapting existing models to new data.