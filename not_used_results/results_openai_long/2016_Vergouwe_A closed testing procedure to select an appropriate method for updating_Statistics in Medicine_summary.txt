## Summary

This paper proposes a **closed testing procedure** to choose how extensively to **update an existing logistic-regression prediction model** when it performs poorly in a new population. The core problem is a trade-off: **more extensive updating** (re-estimating many coefficients) can improve fit in the update sample but **risks overfitting**, especially with limited sample size.

### Updating methods considered (ordered from least to most extensive)
0. **No update**: use the original model as-is.  
1. **Recalibration-in-the-large**: update only the **intercept** to match baseline risk in the new population.  
2. **Recalibration (logistic calibration)**: update **intercept + one global slope** multiplying the original linear predictor (adjusts overall predictor strength).  
3. **Model revision**: re-estimate **all coefficients** using the same predictors (full refit).

### The proposed closed testing procedure
A **sequential likelihood ratio testing** approach that controls the family-wise type I error “approximately” while respecting the natural ordering of updates:

1. Test **full revision vs original** (does *anything* need updating?). If not significant → keep original.
2. If significant, test **full revision vs intercept-only update** (is more than baseline risk shift needed?). If not significant → choose intercept-only update.
3. If significant, test **full revision vs recalibration (intercept+slope)** (are individual coefficients needed beyond a global slope?).  
   - If not significant → choose recalibration (intercept+slope)
   - If significant → choose full revision

This ensures **revision is chosen only when strongly supported**, reducing overfitting.

### Clinical illustrations (3 examples)
1. **Prostate cancer (indolent vs significant cancer)**  
   - Miscalibration mainly due to **baseline risk difference** (calibration intercept ≈ 2.58).  
   - Full revision looked “significant” if tested alone, but showed **overfitting** (optimism-corrected calibration slope ≈ 0.86).  
   - **Closed testing selected intercept-only recalibration**, which fixed the main issue without overfitting.

2. **Traumatic brain injury (6‑month mortality)**  
   - Baseline risk shift small; main issue was **slope > 1** (predictor effects too weak overall in new data).  
   - With enough data, closed testing tended to select **model revision**; in smaller samples it often selected **simpler updates**, reflecting limited evidence.

3. **Febrile children (serious bacterial infection)**  
   - Original model performed reasonably; updates yielded modest gains.  
   - Closed testing selected **model revision** in the full dataset, but often chose the original model or simpler updates in smaller simulated samples.

### Simulation study findings
- **Type I error**: slightly above 0.05 in small samples (≈0.07–0.08 at low events-per-variable), approaching 0.05 as sample size increases.
- **Estimation accuracy**: the closed testing strategy produced **lower mean squared error of coefficients** than always doing full revision, especially in small samples—consistent with reduced overfitting.

### Take-home message
The paper concludes that a **closed testing procedure** provides a practical, statistically principled way to select among common updating approaches—often favoring **parsimonious recalibration** unless there is strong evidence that **individual predictor effects truly changed**—thereby improving transportability while limiting overfitting.